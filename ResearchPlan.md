### 研究计划
制定者：Wenyu
时间：2019/2/21
版本：1.1

### 研究目的：
实现实时（30Hz）远距离（1.2-1.8m）的注视点检测（屏幕注视点（显示器大小不限于1280 x 800），屏幕仍为近距离（60cm））

### 研究内容：
1.	实验范式设计（数据采集范式（时间、交互形式、被试者的自由度）、仪器）
2.	数据预处理方法（剔除错误采样（人工或自动），如何生成训练集，大数据的分布式处理）
3.	方法研究（纯深度学习（从原始图像回归注视点），结合传统方法（标定人脸平面等））
4.	方法分析（采用反卷积等可视化手段对模型进行分析，包括各层重要性、区域重要性等）
5.	泛化模型研究（多个被试者、不同场景的通用模型）
6.	迁移学习研究（在完成以上内容且指标可观的情况下着手开始研究（迁移学习的原模型需求、新被试者的标定或训练数据需求））

### 技术路线：
1.	数据采集：采用眼动仪捕获注视点数据，截获鼠标点击，采用高清摄像头捕获图像，主要解决数据配准和时间同步问题，结合最终实验结果改进实验范式
2.	方法：
2.1	纯深度学习：结合YOLO等，在输出上进行考虑，同时优化其它层
2.2	结合传统方法：采用Face++、dlib等获取脸部特征点从而获取人脸平面，结合标定相机计算头部朝向等作为特征，结合眼部特征点，获取眼部图像，进而利用深度学习回归注视点
2.3	方法分析：采用反卷积神经网络等对模型进行可视化，采用掩膜法对区域重要性进行分析

### 执行标准：
1.	研究规划：按本计划执行
2.	代码标准：尽量完全采用python实现，coding参考google，对于实时性能的提高，可以考虑C、C++，注释采用全英文
3.	代码管理及问题讨论：git，github（push、pull、issue）
4.	工作总结：Poster（English）每周

### 时间规划：（每周迭代向后制定一周计划）
1. 2019/2/18 -- 2019/2/28 解决目前存在的同步问题，通过可视化等手段进行验证，使用反卷积等对第一层CNN进行分析，在其他数据集上验证模型并对比他人论文指标

（Appearance-Based Gaze Estimation in the Wild https://ieeexplore.ieee.org/document/7299081 (Xucong Zhang et al.)，MPIIFaceGaze，角度误差）

（Visualizing and Understanding Convolutional Networks https://arxiv.org/abs/1311.2901v3 (Matthew D Zeiler et al.)）

#### TODO:
WYC: 采集程序加时间戳，匹配程序（可以考虑最小欧氏距离加阈值），可视化程序（将采集到的图像以及注视点显示出来，并标注该帧的时间误差），确定实验场地

XYC: 测试多个人，将CNN第一层输出、第一层反卷积结果可视化出来，同时对应每一个epoch的训练误差和测试误差（均匀划分数据集）

2. 2019/3/1 – 2019/3/7 在时间同步基础上开始大样本数据采集（固定摄像头和显示器的相对位置（这样可以切换场景，也可考虑先尝试独立场景）），使用反卷积等对其它层（全部）进行分析，在其他数据集上验证模型并对比他人论文指标
（Eye Tracking for Everyone https://arxiv.org/abs/1606.05814 (Kyle Krafka et al.)，GazeCapture，角度误差，长度误差）

 
